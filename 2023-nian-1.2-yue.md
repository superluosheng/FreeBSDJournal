####FreeBSD Journal • January/February 2023####

# ZFS的Atomic I/O和PostgreSQL

# 虚拟实验室BSD编程研讨会

# ZFS简介

# WIP/CFT:数据包批处理

## ZFS的Atomic I/O和PostgreSQL
作者：Thomas Munro

PostgreSQL 是一个实施 SQL 标准的关系数据库管理系统，具有类似BSD的许可证。SQL 早期的原型 POSTGRES 于80年代中期在伯克利大学开始。它在FreeBSD上很流行，在那里它通常被部署在ZFS存储上。

许多关于 PostgreSQL 在 ZFS 上的文章建议改变 ZFS 的 recordsize 设置和 PostgreSQL 的 full_page_writes 设置。后者的设置对性能和崩溃安全的真正影响并不经常被解释，也许是因为在大多数流行的文件系统上调整它通常是不安全的。在这篇文章中，围绕块的大小讨论之后我总结了这一神秘机制背后的逻辑和权衡。 

块 几乎所有 PostgreSQL 的磁盘I/O 都是在 8KB 块或页面上对齐的。重新编译它以使用不同的大小，但很少有人这样做。这个大小可能最初是为了匹配 UFS 的历史默认块大小（不过请注意 FreeBSD 的 UFS 现在默认为32KB）。ZFS 使用术语 "记录大小"，并默认为128KB。与其他文件系统不同，ZFS 允许在任何时候轻松改变记录大小，并为每个数据集单独配置。

如果数据是随机访问，那么在理论上，其大小最好与PostgreSQL的 8KB 的块相匹配。否则，随机I/O可能会受到两种影响。

- I/O放大，因为对 8KB 块的每一次读或写都会传输额外的邻近的数据

- 先读后写,当存储块目前不在操作系统的缓存中而一个 8KB 的块必须被写入，所以必须先读取相邻的数据。

如果数据大多数按顺序访问，或者很少访问，尤其是如果使用较大记录的 ZFS 压缩收益超过了对 I/O 带宽和延迟的担忧，那么这可能是一个好主意。

一些来源提出了16KB、32KB或 128KB 的记录大小的全面建推荐，作为在没有太多写入放大或延迟的情况下实现更好压缩的临界点。在这里，我未必有一个答案，而是要说明发生了什么。

有些应用对不同种类的数据多种要求。表空间可以用于将不同的表存储在具有不同记录大小、压缩或物理介质的不同ZFS数据集中。也可以对表进行分区，例如，较旧的数据在一个表空间中，当前活动数据在另一个表中。

‘’‘
CREATE TABLESPACE compressed_tablespace
LOCATION ’/tank/pgdata/compressed_tablespace‘;
ALTER TABLE t
SET TABLESPACE compressed_tablespace;
’‘’

据报道，小的ZFS记录大小的一个问题是碎片化。一个收到
频繁的随机更新的表可能最终会出现散落在各处的块，而我们
我们希望它们在物理上被集中起来，以获得良好的连续读取性能。一个简单的方法
要求PostgreSQL重写保存表及其索引的文件，以便在ZFS级别上对其进行碎片整理。
一个简单的方法是要求PostgreSQL重写存放表及其索引的文件，以便在ZFS级别上对它们进行碎片整理，这就是发布VACUUM FULL table_name或CLUSTER
table_name，如果你准备在重写期间将查询锁定在该表之外。
重写一个表也允许新的记录大小生效，如果它已经在数据集级别上被改变的话。
数据集层面上的改变，重写表也允许新的记录大小生效。

小的 ZFS 记录大小报告的一个问题是碎片。一个频繁接收随机更新的表最终可能会出现分散在各处的块，我们更希望它们在物理上被集中起来，以获得良好的顺序读取性能。如果您准备在重写期间将查询锁定在表之外，那么要求 PostgreSQL 重写包含表及其索引的文件以便在ZFS级别对其进行碎片整理的一种简单方法是发出 `VACUUM FULL table_name` 或 `CLUSTER table_name`。如果在数据集级别更改了新的记录大小，则重写表也可以使其生效。

被撕毁的写入

PostgreSQL 设置 full_page_writes 默认为打开，ZFS用户经常将其关闭。这样写密集型工作负载的性能就会变得更快、更一致。例如，在一个低端云虚拟机上的简单pgbench测试中，我通过关闭它来测量每秒增加32%的事务。

那么它到底有什么作用呢？这需要大量的背景解释。

简短的版本是 PostgreSQL 使用 `physiological logging` 来确保崩溃安全，这意味着在电源故障时对单个数据库页面的写入必须是原子的，否则可能无法在崩溃后恢复。除非您承诺您的存储堆栈具有该属性，否则PostgreSQL必须做一些额外的工作来保护您的数据。

断电原子性是这样一种特性，即如果断电时物理写入正在进行，那么对于某些给定的块大小，稍后我们可以预期读取块的旧版本或新版本，但不能读取部分修改或撕裂的版本。不要将其与并发读写的原子性混淆（见下文）。`physiological logging`，是物理到页面、页面内逻辑的缩写，是教科书中记录策略分类的一个术语，它意味着日志记录通过文件和块号识别要更改的块，但随后在注释中描述要在该页面内进行的更改，这需要我们读入现有页面，以了解如何“逻辑上”修改它，而不仅仅是更新物理地址的位。

崩溃后，恢复算法可以处理“旧”页面内容或“新”页面内容，应用所需的任何日志更改以使其更新。如果遇到新旧数据的非原子混合，则无法重新播放对页面的逻辑更改，并且恢复失败！一个表面上的问题是，如果启用了data_checksums，那么PostgreSQL的页面级校验和检查将失败，甚至无法读取页面。如果禁用了校验和，我们将进一步了解，但诸如“在插槽3中插入元组（42，Fred）”之类的逻辑更改无法可靠地回放。为了在本例中应用更改，我们需要了解一个使用页面上预先存在的元数据的插槽表，但它可能已损坏。

`physiological logging` 是数据库行业中应用非常广泛的技术，不同的RDBMS已经找到了不同的解决页面撕裂问题的方法。由于开源系统已经被开发并用于各种各样的低端系统，通常没有各种形式的硬件保护来防止电源损失，因此故障很常见，必须开发软件解决方案。

PostgreSQL目前的解决方案是切换到页面级仅 `physiological logging` 或整页写入，将整个数据页转储到日志中，以便在每个检查点之后对每个数据页进行第一次修改。检查点是一种周期性的后台活动，在理想的情况下，对前台事务性能的影响微乎其微。然而，由于第一次接触规则，一旦检查点启动，写繁重的工作负载可能会突然开始生成更多的日志数据，因为小的更新突然需要记录许多 8KB 的页面。这种影响通常会逐渐减弱，因为对每个页面的后续修改会回到 `physiological`，直到下一个检查点，有时会导致I/O带宽和事务延迟出现锯齿状模式。

另一个流行的开源数据库有一个不同的解决方案，它还包括将所有数据写两次，并在两次之间设置同步屏障，因为这两个副本都不能被撕毁。

ZFS不需要这些！由于它自己的写时复制设计具有记录级的原子性。不可能看到 ZFS 记录的新旧内容的混合，因为它不会物理覆盖它们，而且它的TXG和ZIL系统使写入成为事务性的。因此，只要记录大小至少为8KB，就可以安全地将full_page_writes设置为off。

请注意，在某些场景中，ZFS 本身也会物理地写入两次数据。一个常见的建议是考虑为包含主要数据文件的数据集设置logbias=吞吐量（但可能不是本文中未探讨的包含PostgreSQL日志目录pg_wal-A-top-ic的数据集）。该选项尝试将块直接写入它们的最终位置，而不是首先将它们记录在ZIL中。如果您使用 ZFS 默认的 logbias=latency 和 PostgreSQL 默认的full_page_writes=on，那么实际上数据可能总共被写了四次，因为 PostgreSQL 和 ZFS 都需要执行额外的工作来创建记录级的一致性，而这两个更改都会将其减少到一个副本。

不幸的是，在两种特殊情况下，仍然需要 full_page_writes=on 才能进行正确的行为：在运行 pg_basebackup 和 pg_rewind 时。这些工具用于备份，或从另一个服务器创建或重新同步流复制副本；在 pg_basebackup 的情况下，在运行该命令时将以静默方式启用全页写入，而在 pg_rewind 的情况中，如果未手动启用该命令，则该命令将拒绝运行（当前版本中存在令人讨厌的不一致性）。这些工具制作数据文件的原始文件系统级副本，以及崩溃恢复所需的日志，以处理由并发更改引起的一致性问题。在这里，我们遇到了I/O原子性的另一个含义：从可能并发写入的文件中读取。第一个问题是，当存在重叠的并发写入时，Linux和Windows上的文件系统（但不是ZFS，或FreeBSD上的任何文件系统，因为使用了范围锁）可以向读者显示随机选择的前位和后位。此外，I/O目前是以一种不适当对齐的方式进行的，因此即使在ZFS上，也可能复制被撕裂的页面。为了防止这种情况，需要使用full_page_writes行为。这个问题最终应该在  PostgreSQL 中得到解决，方法是通过适当的对齐和互锁复制原始数据文件。请注意，如果采取了某些预防措施（主要是快照必须原子化地捕获日志和所有数据文件），可以使用ZFS快照而不是 pg_basebackup，从而减少克隆或备份一个非常繁忙的系统时的影响。


我们已经看到 full_page_writes=off 如何提高写事务的性能，而 ZFS 使其变得安全。不幸的是，复制和故障恢复也可能带来负面的性能影响。这两个活动都执行恢复，意味着它们会回放日志。尽管全页镜像在编写时令人讨厌，但在恢复时回放时，它们起到了优化的作用。我们不需要执行可能会阻止恢复的串行处理循环的随机同步读取，而是在我们良好的顺序日志中已经有了要修改的页面内容，然后将其缓存。

PostgreSQL 15包含了这个问题的部分解决方案：它在日志中查找即将被读取的页面，并发出 POSIX_FADV_WILLNEED 建议，以生成可配置的 I/O 并发度（一种穷人的异步I/O）。在撰写本文时，FreeBSD 忽略了这一建议，但未来版本的 OpenZFS 有望将其连接到 FreeBSD 的 VFS（OpenZFS拉取请求#13958）。最终，这应该被一个真正的异步I/O子系统所取代，该子系统目前正在开发中，并被提议用于未来版本的 PostgreSQL。

一个小组在 illumos 操作系统的ZFS上大规模使用 PostgreSQL，研究了 full_page_writes=off 对恢复I/O暂停的影响。他们开发了一种名为 pg_prefaulter 的工具作为变通方法。他们发现，由于可预测的I/O暂停，他们的流式复制副本无法跟上主服务器的速度。由于 PostgreSQL 的大多数大规模用户甚至没有设置full_page_writes=off的选项，因此它们可能是唯一能够看到这种效果的。如果遇到这个问题，pg_prefaulter 可能是一个解决方案，直到内置预取可用。

展望未来
在未来的 PostgreSQL 版本中，区块大小对齐可能会成为一个更大的话题，希望包括提议的直接I/O支持，目前只存在于原型形式。这与 OpenZFS（pull request #10018）的直接I/O支持的发展不谋而合，这可能需要块大小的一致才能有效地工作（目前的原型恢复到ARC，否则；一些其他的文件系统只是拒绝非对齐的直接I/O）。另一个对数据库非常有用的OpenZFS功能是块克隆（pull request#13392），以及FreeBSD的新系统接口，PostgreSQL应该能够使用它来快速克隆数据库和数据库对象，其粒度比整个数据集更细。

THOMAS MUNRO是一名为Microsoft Azure工作的开源数据库黑客，他通常登录到FreeBSD盒子中。


## 虚拟实验室BSD编程研讨会
作者：ROLLER ANGEL

接口。即使我们没有多个物理网络接口，我们也可以始终制作一个单独的网络接口来与cloned_interfaces一起使用。就我而言，我正在重新调整一些笔记本电脑的用途。其中一个有一个可以使用的WiFi卡，我用它来连接互联网。这是一台2010年代早期的华硕ROG笔记本电脑。它还有一个内置的以太网端口。在我的X1 Carbon第7代ThinkPad上，没有内置以太网端口。我只使用USB端口和我的Android USB Tethering。然而，我最兴奋的是戴尔Precision笔记本电脑。这是一台很棒的实验室主机，因为它配备了英特尔至强处理器，可以处理128GB的RAM和多个NVME硬盘。它在笔记本电脑的背面有一个内置的以太网端口，第二个是一个USB-C加密狗，上面有一个Intel igb0以太网端口，我用它作为实验室网络的专用接口。最后，旧的塔作为主机也很好，有一些很棒的PCI网卡，你可以安装多个以太网端口。这里的关键是找到适合您的方法，安装FreeBSD并开始使用我们的虚拟实验室。如果您想获得一些关于安装和配置FreeBSD的提示，请参阅2022年7月/8月FreeBSD期刊上的FreeBSD入门研讨会。

FreeBSD主机
这个主机将需要一些网络接口。我们希望主机能有自己的方式来连接互联网。这可以是一个由本地路由器提供的良好的DHCP分配的地址，或者你的主机可以扮演路由器的角色，通过一个调制解调器直接连接到外部世界。无论你的主机如何获得互联网连接，我们都希望有一个额外的网络接口，只用于我们的实验室网络。这个接口的名称可能是em0、igb0或类似的，这取决于网卡驱动程序和安装的接口数量。即使我们没有多个物理网络接口，我们也可以单独做一个网络接口来使用cloned_interfaces。在我的例子中，我正在重新利用一些笔记本电脑。其中一台有一块可用的WiFi卡，我用来连接互联网。这是一台2010年代早期的华硕ROG笔记本电脑。它也有一个内置的以太网端口。在我的X1 Carbon第七代ThinkPad上，没有内置的以太网端口。我只是使用USB端口和我的安卓USB Tethering。然而，我对戴尔Precision笔记本电脑最感兴趣。它是一个伟大的实验室主机，因为它配备了英特尔至强处理器，可以处理128GB的内存和多个NVME硬盘。它的笔记本后面有一个内置的以太网端口，第二个是USB-C加密狗，上面有一个英特尔igb0以太网端口，我用它作为实验室网络的专用接口。最后，旧塔楼也可以作为主机使用，有一些神奇的PCI网卡，你可以安装多个以太网端口。这里的关键是找到适合你的东西，安装FreeBSD并开始使用我们的虚拟实验室。如果你想获得一些安装和配置FreeBSD的技巧，请看2022年7/8月的FreeBSD杂志中的FreeBSD入门研讨会。

虚拟网络设计
就像在物理服务器世界中，我们在路由器/防火墙系统上有多个物理以太网
端口，我们可以为我们的实验室路由器/防火墙系统提供多个接口。
我们可以为我们的实验室路由器/防火墙系统提供多个接口，从现在开始我将其称为网关。这些接口将通过网桥相互连接。想想看，一个以太网
交换机，网桥提供了类似的功能。而对于我们的
以太网电缆，我们将使用epair(4)接口。这包括两边，A边和B边。
我们将A端连接到网桥，B端连接到我们的监狱。这样，所有的监狱都可以通过虚拟网桥相互通信，就像网络中的物理主机通过以太网电缆连接到交换机一样，可以相互通信。网关监狱将有一条额外的虚拟电缆连接到一个单独的网桥，允许连接到实验室外和互联网上。所有监狱都将设置其默认路由，以指向该网关。我们给这个独立的网桥提供连接到外部世界的能力的方法是通过指定一个物理接口作为网桥的成员。同样， 这类似于把以太网电缆插入交换机， 但在这种情况下， 以太网电缆被插入 FreeBSD 主机系统， 然后被虚拟插入网桥。通过这样做，同为网桥成员的其他虚拟以太网线将能够与之通信，并使其数据包在实验室环境中流动。
让我们把它分解一下，每个监狱使用一种叫做vnet的技术获得自己的网络。我们将FreeBSD主机上的物理接口连接到一个虚拟网桥上，并为我们的实验室网络创建第二个虚拟网桥，该网桥上只有来自实验室监狱的虚拟接口与之相连。然后我们使用路由和防火墙规则来推送数据包，因为我们认为合适。

防火墙
我们将使用PF作为防火墙。FreeBSD Jails的工作方式是，每个监狱都共享主机内核，所以如果有一个内核模块你想在监狱里访问，你只需要允许它，然后在特定监狱的jail.conf设置中配置访问。看一下/etc/defaults/devfs.rules文件。为了让我们的网关使用PF防火墙，我们需要将监狱的配置设置为使用该文件中列出的pf规则集。我们稍后将设置自己的自定义devfs规则，并包括devfsrules_jail_vnet规则中的配置。

配置
现在我们已经涵盖了我们要做的事情，让我们开始做吧。我们从一个运行13.1-RELEASE的FreeBSD主机开始。如上所述，这个主机应该有一个活跃的互联网连接。我们将使用这个连接来下载一些文件，以用于创建我们的监牢。该主机正在运行NTPD，所以它能获得准确的时间。用sockstat -46检查在主机上监听的任何服务，如果没有使用，就把它关掉。记住，主机应该限制它所做的事情--在我们的实验室里，我们会有很多有趣的事情要做，所以尽你所能限制主机上的服务。我打算亲自用附带的键盘和屏幕登录来管理我的主机，所以我没有在主机上启用SSH。
现在我们已经准备好启用监狱了。Asimplesysrc jail_enable=YESwilldethetrick.不需要安装任何软件包，监狱管理已经内置于FreeBSD。看看/usr/share/examples/jails中的README文件，了解一些如何配置监狱的例子。正如你所看到的，有很多方法可以进行监狱配置。我已经做了研究，并挑选了你将在这里看到的配置方法。欢迎你尝试一下其他的方法，看看有什么合适的。如果你真的用其他方法来完成这项任务，请考虑把它写下来，这样别人就可以看到你发现的有用方法，并自己尝试一下。在这一点上，我们已经验证了我们的主机已经为托管监狱做好了准备，并且已经启用了监狱服务，所以我们可以快速重启，仔细检查主机上是否有最小的服务在监听，然后继续创建我们所有监狱要使用的基本配置。在编辑配置文件时，我们将使用vim，对于基本的编辑任务，你真的只需要知道一些东西，花几分钟时间通过vimtutor命令中的交互式练习来了解你的方向，你将在短时间内成为vim的新手。
注意：我们在运行以下所有的命令时都是以root用户的身份进行的。

###edit jail.conf
     vim /etc/jail.conf

###put the following into /etc/jail.conf
3 of 9
 
$labdir=”/lab”;
$domain=”lab.bsd.pw”;
path=”$labdir/$name”;
host.hostname=”$name.$domain”;
exec.clean;
exec.start=”sh /etc/rc”;
exec.stop=”sh /etc/rc.shutdown”;
exec.timeout=90;
stop.timeout=30;
mount.devfs;
exec.consolelog=”/var/tmp/${host.hostname}”;

###base.txz
mkdir -p /lab/media/13.1-RELEASE
cd /lab/media/13.1-RELEASE
fetch http://ftp.freebsd.org/pub/FreeBSD/releases/amd64/13.1-RELEASE/base.txz

###Gateway Jail
mkdir /lab/gateway
tar -xpf /lab/media/13.1-RELEASE/base.txz -C /lab/gateway

###edit jail.conf
vim /etc/jail.conf

###add to the bottom of the file
 gateway {
  ip4=inherit;
}

###feel free to add a user account to the jail with the following optional command, for this article we’re just going to be using the user root
chroot /lab/gateway adduser
###set the root password for the jail
chroot /lab/gateway passwd root
###setup DNS resolution using OpenDNS servers
vim /lab/gateway/etc/resolv.conf
###add the following lines to resolv.conf copy the hosts time zone setting
nameserver  208.67.222.222
nameserver  208.67.220.220

###copy the hosts time zone setting
cp  /etc/localtime  /lab/gateway/etc/

###create an empty file system table
touch /lab/gateway/etc/fstab

###start jail
jail  -vc  gateway
###login to jail
jexec  -l  gateway  login  -f  root
###logout of the jail
logout
###list jails
jls
###stop the jail
jail  -vr  gateway
###create devfs.rules
vim  /etc/devfs .rules
###add the following lines to devfs.rules
[devfsrules_jail_gateway=666]
add  include  $devfsrules_jail_vnet
add  path   'bpf*'  unhide
###restart devfs
service  devfs  restart
###verify devfs rules
devfs  rule  showsets
###assign our new ruleset to the gateway jail
vim  /etc/jail .conf
###add the following line to the gateway { } config block
devfs_ruleset=666;
###restart the gateway jail
service  jail  restart  gateway
###verify ruleset was applied to gateway jail
jls  -j  gateway  devfs_ruleset
###we expect to see 666 as the output of the above command

###load the PF kernel module on host
sysrc  -f  /boot/loader .conf  pf_load=YES kldload  pf

enable PF on gateway jail

sysrc  -j  gateway  pf_enable=YES
edit pf.conf on gateway jail
vim  /lab/gateway/etc/pf .conf
add the following config
ext_if  =  “e0b_gateway”
int_if  =  “e1b_gateway”
table  <rfc1918>  const  {  192 . 168 .0 .0/16,  172 . 16 .0 .0/12,  10 .0 .0 .0/8  }

###Allow  anything  on  loopback
set  skip  on  lo0

###Scrub  all  incoming  traffic
scrub  in
no  nat  on  $ext_if  from  $int_if:network  to  <rfc1918>

###NAT  outgoing  traffic
nat  on  $ext_if  inet  from  $int_if:network  to  any  ->  ($ext_if:0)

####Reject  anything  with  spoofed  addresses
antispoof  quick  for  {  $int_if,  lo0  }  inet

####Default  to  blocking  incoming  traffic,  but  allowing  outgoing  traffic block  all
pass  out  all

###Allow  LAN  to  access  the  rest  of  the  world
pass  in  on  $int_if  from  any  to  any
block  in  on  $int_if  from  any  to  self

###Allow  LAN  to  ping  us
pass  in  on  $int_if  inet  proto  icmp  to  self  icmp-type  echoreq


配置虚拟网络
设置一个接口。这里我们使用名为alc0的专用网卡，并将其命名为lab0，作为签名。所有进一步的配置都将使用接口名称lab0，它被分配到的实际物理设备可以通过编辑一行配置来改变。这样，如果我们把实验室搬到另一台主机上，或者在以后添加一个新的网络接口，并使用不同的名称，如igb0、re0或em0，就很容易切换我们的接口。

sysrc  ifconfig_alc0_name=lab0
sysrc  ifconfig_lab0=up
service  netif  restart

###copy the Jail Interface Bridge automation script into our lab scripts directory and make it executable
mkdir  /lab/scripts
cp  /usr/share/examples/jails/jib  /lab/scripts/
chmod  +x  /lab/scripts/jib
edit jail.conf
vim  /etc/jail .conf

网关 jail.conf
在这一点上，我们准备从继承主机的ip4网络转向使用vnet，从/etc/jail.conf中删除网关{}配置块，用以下内容代替

gateway  {
vnet;
vnet .interface=e0b_$name,  e1b_$name;
exec .prestart+=”/lab/scripts/jib  addm  $name  lab0  labnet”;
exec .poststop+=”/lab/scripts/jib  destroy  $name”;
devfs_ruleset=666;
}

为实验室中的监狱创建内部局域网网络
sysrc  cloned_interfaces=vlan2
sysrc  ifconfig_vlan2_name=labnet
sysrc  ifconfig_labnet=up
service  netif  restart

###destroy and recreate gateway
jail  -vr  gateway
jail  -vc  gateway

###configure networking for gateway jail
sysrc  -j  gateway  gateway_enable=YES
sysrc  -j  gateway  ifconfig_e0b_gateway=SYNCDHCP
sysrc  -j  gateway  ifconfig_e1b_gateway=”inet  10 .66 .6 . 1/24”

###service  jail  restart  gateway
jexec  -l  gateway  login  -f  root

###test connectivity
host  bsd .pw
ping  -c  3  bsd .pw
###exit the jail
logout

###create another jail that only has one interface that’s attached to the labnet LAN net- work
vim  /etc/jail .conf
###add the following to the bottom of the file
client1  {
vnet;
vnet .interface=”e0b_$name”;
exec .prestart+=”/lab/scripts/jib  addm  $name  labnet”;
exec .poststop+=”/lab/scripts/jib  destroy  $name”;
devfs_ruleset=4;
depend=”gateway”;
}
make the directory structure for the new jail
mkdir  /lab/client1
tar  -xpf  /lab/media/13 . 1-RELEASE/base .txz  -C  /lab/client1
###set the root password
chroot  /lab/client1  passwd  root
###setup DNS resolution using OpenDNS servers
vim  /lab/client1/etc/resolv .conf
###add the following lines to resolv.conf
nameserver  208.67.222.222
nameserver  208.67.220.220
###copy the hosts time zone setting
cp  /etc/localtime  /lab/client1/etc/
###create an empty file system table
touch  /lab/client1/etc/fstab
start jail
jail  -vc  client1
###configure networking for client1 jail
sysrc  -j  client1  ifconfig_e0b_client1=”inet  10 .66 .6 .2/24”
sysrc  -j  client1  defaultrouter=”10 .66 .6 . 1”
###service  jail  restart  client1
sysrc  -j  client1  ifconfig_e0b_client1=”inet  10 .66 .6 .2/24”
sysrc  -j  client1  defaultrouter=”10 .66 .6 . 1”
service  jail  restart  client1

###login to jail
jexec  -l  client1  login  -f  root

###test connectivity
host  bsd .pw
ping  -c  3  bsd .pw
ping  -c  3  10.66.6.1
###grab a sample tcsh profile
fetch  -o  .tcshrc  http://bsd .pw/config/tcshrc
chsh  -s  tcsh
###exit the jail
logout

下次登录时，由于tcshrc的设置，你会有一个绿色的提示，享受吧      现在你有了一个虚拟实验室，它有自己的虚拟网络，有一个物理接口用于向外连接互联网。由于我们把这个接口命名为lab0，我们可以很容易地更新它。来吧，试一试。例如，你可以插入一个有互联网连接的安卓手机，WiFi或蜂窝电话都可以，因为两者都可以。在你把手机插入主机后，进入手机的网络设置，启用USB连接。现在将有一个ue0接口可供使用。将/etc/rc .conf中的ifconfig_alc0_name="lab0 "一行更新为ifconfig_ue0_name="lab0"。重新启动。登录到任一监狱并测试连接性。你的网络已经被换掉了。你的实验室现在可以移动了
我希望你在跟随这篇文章的过程中感到愉快。我对FreeBSD有着极大的热情，与他人分享我所学到的东西给我带来了快乐。我希望你能在你的实验室里用FreeBSD做一些令人惊奇的事情，我期待着在一个奇妙的BSD会议上与你们中的许多人交谈。

ROLLER ANGEL大部分时间都在帮助人们学习如何使用技术来实现他们的目标。他是一个狂热的FreeBSD系统管理员和Python爱好者，喜欢学习用开源技术--特别是FreeBSD和Python--来解决问题的神奇之处。他坚信，人们可以学习任何他们想学的东西。Roller一直在寻求创造性的解决问题的方法，并乐于接受良好的挑战。他有学习的动力和动机，探索新的想法，并保持他的技能。他喜欢参与研究社区并分享他的想法。



## ZFS简介
作者：Drew Gurkowski

ZFS将卷管理器和独立文件系统的角色合二为一，比独立的文件系统具有多种优势。它以速度、灵活性而闻名，最重要的是，它非常注意防止数据丢失。许多传统的文件系统不得不一次只存在于一个磁盘上，而ZFS知道磁盘的底层结构，并创建一个可用的存储池，甚至在多个磁盘上。当额外的磁盘被添加到池中时，现有的文件系统将自动增长，立即成为文件系统的可用部分。

开始使用
FreeBSD 可以在系统初始化时挂载 ZFS 池和数据集。要启用它，请在 /etc/rc.conf 中添加这一行。

zfs_enable=”YES”
###Then start the service:
service  zfs  start

识别硬件
在设置ZFS之前，确定与系统相关的磁盘的设备名称。做到这一点的一个快速方法是用

#  egrep   da[0-9] |cd[0-9]    /var/run/dmesg .boot

输出应确定设备名称，在本指南的其余部分中的例子 将使用默认的SCSI名称：da0、da1和da2。如果硬件不一样，请确保使用正确的设备名称来代替。

创建一个单磁盘池
要使用单个磁盘设备创建一个简单的、非冗余的池。

#  zpool  create  example  /dev/da0

要创建文件供用户在池内浏览。
#  cd  /example
#  ls
#  touch  testfile

可以用ls查看新文件。
#  ls  -al

我们已经可以开始使用更高级的ZFS功能和属性。要在池中创建一个启用了压缩功能的数据集。

#  zfs  create  example/compressed
#  zfs  set  compression=on  example/compressed

example/compressed数据集现在是一个ZFS压缩文件系统。
用以下方法禁用压缩。
#  zfs  set  compression=off  example/compressed

要卸载一个文件系统，使用zfs umount，然后用df验证。

#  zfs  umount  example/compressed
#  df

确认example/compressed没有被列为输出下的一个挂载文件。该文件系统可以用zfs重新挂载。
#  zfs  mount  example/compressed
#  df

在文件系统被挂载的情况下，输出应该包括与下面类似的一行。
example/compressed  17547008  0  17547008  0%  /example/compressed

ZFS数据集的创建就像其他文件系统一样，下面的例子创建了一个名为data的新文件系统。
#  zfs  create  example/data

使用df查看数据和空间使用情况（为了清晰起见，已经删除了部分输出）。
#  df

example/    compressed  17547008        0   17547008        0%  /example/compressed
example/    data        17547008        0   17547008        %   /example/data

因为这些文件系统是建立在ZFS上的，它们从同一个存储池中提取。这消除了对其他文件系统所依赖的卷和分区的需要。

要销毁文件系统，然后销毁不再需要的池。
#  zfs  destroy  example/compressed
#  zfs  destroy  example/data
#  zpool  destroy  example

RAID-Z
RAID-Z池需要三个或更多的磁盘，但在一个磁盘发生故障时提供数据丢失的保护。因为ZFS池可以使用多个磁盘，对RAID的支持是文件系统设计中固有的。
要创建一个RAID-Z池，指定要添加到池中的磁盘。

#  zpool  create  storage  raidz  da0  da1  da2

在创建了zpool之后，可以在该池中制作一个新的文件系统。
#  zfs  create  storage/home

启用压缩功能，并存储一个额外的目录和文件副本。
#  zfs  set	copies=2  storage/home
#  zfs set compression=gzip  storage/home

一个RAID-Z池是一个存储关键系统文件的好地方，例如用户的主目录。
#  cp  -rp  /home/*  /storage/home
#  rm  -rf  /home  /usr/home
#  ln  -s  /storage/home  /home
#  ln  -s  /storage/home  /usr/home

可以创建文件系统快照，以便以后回滚，快照名称用黄色标记，可以是你想要的任何名称。
#  zfs  snapshot  storage/home@ 11-01-22

ZFS创建数据集的快照，允许用户备份文件系统，以便将来进行回滚或数据恢复。
#  zfs  rollback  storage/home@ 11-01-22

要列出所有可用的快照，可以使用zfs list。
#  zfs  list  -t  snapshot  storage/home

恢复RAID-Z
每个软件RAID都有一个监控其状态的方法。使用以下方法查看RAID-Z的状态。
#  zpool  status  -x

如果所有池子都是在线的，而且一切正常，则显示消息。
all  pools  are  healthy

如果有一个问题，也许是一个磁盘处于离线状态，池的状态将看起来像这样。
pool:  storage
state:  DEGRADED
status:  One  or  more  devices  has  been  taken  offline  by  the  administrator .      Sufficient  replicas  exist  for  the  pool  to  continue  functioning  in  a degraded  state .
action:  Online  the  device  using   ‘zpool  online’  or  replace  the  device  with 'zpool  replace' .
scrub:  none  requested
config:

     NAME      STATE     READ WRITE     CHKSUM
     storage   DEGRADED  0    0         0
     raidz1    DEGRADED  0    0         0
          da0  ONLINE    0    0         0
          da1  ONLINE    0    0         0
          da2  ONLINE    0    0         0


errors:  No  known  data  errors
“OFFLINE”shows the administrator took da1 offline using:
#  zpool  offline  storage  da1

立即关闭计算机电源并更换da1：打开计算机电源并返回到池中的da1：
#  zpool  replace  storage  da1

接下来，再次检查状态，这次不使用-x来显示所有池：
#  zpool  status  storage
pool:  storage
state:  ONLINE
scrub:  resilver  completed  with  0  errors  on  Fri  Nov  4  11:12:03  2022 config:

     NAME      STATE     READ WIRTE     CKSUM
     storage   ONLINE    0    0         0
     raidz1    ONLINE    0    0         0
     da0       ONLINE    0    0         0
     da1       ONLINE    0    0         0
     da2       ONLINE    0    0         0


errors:  No  known  data  errors

数据验证
ZFS 使用校验和来验证存储数据的完整性，可以验证这些数据校验和(称为清理)以确保存储池的完整性。
#  zpool  scrub  storage
由于大量的输入/输出要求，一次只能运行一次清理。擦除的长度取决于池中存储的数据量。清理完成后，使用 `zpool status` 状态查看状态。

#  zpool  status  storage
pool:  storage
state:  ONLINE
scrub:  scrub  completed  with  0  errors  on  Fri  Nov  4  11:19:52  2022
config:
     NAME      STATE     READ WRITE     CKSUM
     storage   ONLINE    0    0         0
     raidz1    ONLINE    0    0         0
     da0       ONLINE    0    0         0
     da1       ONLINE    0    0         0
     da2       ONLINE    0    0         0


rors:  No  known  data  errors


显示上次清理的完成日期有助于决定何时开始另一次清理。常规 scrub 有助于保护数据免受静默式损坏，并确保池的完整性。


ZFS管理
ZFS有两个主要的管理实用程序：zpool实用程序控制池的操作，并允许添加、删除、替换和管理磁盘。zfs实用程序允许创建、销毁和管理数据集，包括文件系统和卷。
虽然本入门指南不涉及 ZFS 管理，但您可以参考ZFS（8）和zpool（8）了解其他ZFS选项。

其他资源
虽然使用本指南创建的非冗余池和 RAID-Z 池在大多数用例中都可以工作，但更复杂或更专业的系统可能需要进一步的ZFS管理和设置。ZFS 是一个功能强大且可自定义的文件系统本指南几乎没有触及使用它可以做什么的表述
。`OpenZFS wiki` 有大量关于安装、ZFS 系统管理和手册页面的文档。如果由于系统体系结构的原因需要进行调优，那么可以在 OpenZFS 和 FreeBSD 的wiki 页面上找到 ZFS 调优指南。


## WIP/CFT:数据包批处理
作者：汤姆·琼斯和约翰·鲍德温

在过去的30年里，我们使用的计算机增长速度之快令人难以想象。1995年Alpha AXP的论文谈到了机器的设计，它延续了前25年的趋势，速度提高了1000倍。
我们当然已经实现了这个目标，作为FreeBSD最初目标的386机器类似于我们今天在键盘中使用的微控制器。

即使有了这些变化，计算机性能的核心仍然保持不变，每单位工作执行的指令更少，而且速度会更快。这种基本的网络事实导致了几种不同的方法来提高性能。我们已经研究了将工作从CPU转移到带有校验和卸载的网卡的机制。如果卡运行指令来校验输出的数据包，那么我们宝贵的CPU时间可以用来做其他事情。

校验和卸载看到了很好的结果，我们开始将其他东西从CPU转移到网络接口。TCP分段卸载（TSO）是提高网络发送器性能的下一个伟大机制。我们可以形成一个模板，并将其与一大块数据一起发送到卡上，而不是为我们将要发送的TCP段形成IP数据包。网络接口在将数据包放置到导线上时处理分段。TSO为TCP发送器带来了巨大的好处，使我们能够在耗尽单个核心之前就使10千兆网络接口饱和。

TSO让我们能够利用宝贵的资源提高效率。我们减少了发送每个数据包所需的总线（内存和PCI）事务的数量，方法是将它们批处理在一起，并在传输点创建最终块。对于TCP来说，这是直接的，大多数时候，如果我们正在批量发送数据流，并且数据块是清晰的。为了在TCP接收器上反映这些改进，我们有大型接收卸载（LRO）。LRO使我们能够再次减少维持高速数据传输所需的事务数量。

对于UDP，Linux有一些通用机制，试图复制类似TSO的机制。此支持由通用分段卸载（GSO）和通用接收卸载（GRO）一起提供。GSO在订单上实现了巨大的改进，对于UDP发送器来说，可以提高20%，GRO更难测量，但机制是存在的。

FreeBSD对TSO和LRO有很好的支持，但缺乏类似于GSO和GRO的机制。去年在维也纳的EuroBSDCon，我与John Baldwin谈论了他正在研究的一种类似于GRO的机制，他称之为Packet Batching。

TJ：数据包批处理工作的背景是什么？
JB：在接收时进行数据包批处理的想法已经存在了一段时间，至少以愿望列表项目的形式存在，我听过很多人多次提到。我们已经有一些特定于TCP的用于发送（TSO）和接收（LRO）的数据包批处理形式。这种数据包批处理旨在比LRO更通用，以便它可以应用于其他协议（主要是UDP）。
TJ：为什么需要做这项工作？
JB：TSO和LRO等数据包批处理方法的目标是通过每批进行一次而不是每包进行一次来摊销每包成本（网络堆栈中对报头字段的各种检查等）。随着网络速度的增长速度快于CPU速度，每包开销的成本成为一个越来越严重的问题。的确，通常情况下，解决这个问题的方法之一是通过使用RSS在绑定到不同CPU的单独队列中分发数据包来进行横向扩展，这确实有助于解决每个数据包的开销。然而，您不能在多个核心之间分配单个流，批处理方案旨在提高单个队列的性能。
TJ：这项工作使哪些新功能/增强成为可能？
JB：目标是提高PPS和/或减少网络接收工作负载的CPU使用量。当启用LRO时，我不希望它对TCP有帮助，主要是对UDP有帮助。
TJ：人们如何测试工作？通常，我们需要强调使用更多的分布式工作负载进行测试，这在这里适用吗？
JB：欢迎进行基准测试。我最初使用iperf3的一组简单基准测试是好坏参半的，并且没有足够明显的胜利来证明这些更改是合理的。这些变化确实增加了复杂性，所以我认为，在将其视为提交候选之前，它需要在某些工作负载中取得明显的胜利。到目前为止，我还没有观察到我的基准出现任何倒退，只是测量到零收益。
TJ：你希望得到什么样的反馈？
JB：直接给我发电子邮件可能是目前发送反馈的最佳方式。在未来的某个时刻，我将在net@和/或arch@上启动一个公共RFC线程，届时该线程将是发送反馈的最佳位置。想要测试补丁或查看补丁的人可以在https://github.com/freebsd/freebsd-src/compare/main...bsdjhb:-freebsd:cxgbe_batching。
从John在这里的回应来看，还不清楚应该从哪里看到好处。iperf3测量无法模拟非常繁忙的服务器的工作负载。对于FreeBSD中的数据包批处理来说，可能需要测试和调整更多的工作负载。通过关闭John的github分支并尝试您的网络流量，您可以帮助在FreeBSD中建立新的接收器优化。
TOM JONES希望基于FreeBSD的项目得到应有的关注。他住在苏格兰东北部，提供FreeBSD咨询服务。
JOHN BALDWIN是一名系统软件开发人员。20年来，他直接致力于在内核的各个部分（包括x86平台支持、SMP、各种设备驱动程序和虚拟内存子系统）和用户空间程序中对FreeBSD操作系统进行更改。除了编写代码，John还在FreeBSD核心和发布工程团队中任职。他还为GDB调试器和LLVM做出了贡献。John和他的妻子Kimberly以及三个孩子Janelle、Evan和Bella住在加利福尼亚州的康科德。